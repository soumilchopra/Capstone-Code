{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a8c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumilchopra/.pyenv/versions/3.9.4/lib/python3.9/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy  \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d707a54-d527-4cbd-9c9d-5f71c244d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wiki.pathmind.com/neural-network#define\n",
    "# deep learning counts as a model with 3 or more layers (its not just a buzz word)\n",
    "# input * weight = guess \n",
    "# truth - guess = error\n",
    "# error * weight's contribution to error = adjustment \n",
    "# Essentially: scoring input, calculating loss, and applying an update to the model, \n",
    "# then begin that three step process again\n",
    "\n",
    "# Imagine multiple linear regression is happening at every node of a neural network. \n",
    "# For each node of a single layer, input from each node of the previous layer is recombined \n",
    "# with input from every other node. That is, the inputs are mixed in different proportions, \n",
    "# according to their coefficients, which are different leading into each node of the subsequent\n",
    "# layer. In this way, a net tests which combination of input is signiicant as it tries to reduce error.\n",
    "\n",
    "# Once you sum your node inputs to get a prediction on Y value, it is passed through a non-linear function\n",
    "# Otherwise with each layer y would just increase, which is not what we want\n",
    "# Nonlinear transforms are usually: tanh, hard tanh etc. (S-shaped functions similar to logistic regression)\n",
    "\n",
    "# Each weight is just one factor in a deep network that involves many transforms; \n",
    "# the signal of the weight passes through activations and sums over several layers, so we use \n",
    "# chain rule of calculus to march back through the networks activations and outputs and finally\n",
    "# arrive at the weight in question, and its relationship to overall error.\n",
    "\n",
    "# Logistic regression is used for classification rather than regression in the linear sense that most people\n",
    "# are familiar with. It calculates the probability that a set of inputs match the label\n",
    "\n",
    "# F(x) = 1/(1 + e^(-x))\n",
    "# As the input x that triggers a label grows, the expression e to the x shrinks toward zero leaving = 1\n",
    "# As the input x correlates negatively, the negative gets fliped and e to the x grows larger leaving = 0\n",
    "# X = sum of the products of all the weights and their corresponding inputs (total signal of NN)\n",
    "# then we can set a decision threshold to classify as true/false\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b117d3-6cd9-49fb-90ac-f7d59954cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 0., 1.], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing and Splitting the data into training and testing sets for both features and labels \n",
    "df_normal = pd.read_csv('ptbdb_normal.csv')\n",
    "df_abnormal = pd.read_csv('ptbdb_abnormal.csv')\n",
    "df_combined = pd.concat([df_normal, df_abnormal]) # combining both abnormal and normal datasets\n",
    "\n",
    "df_train, df_test = train_test_split(df_combined, test_size = .2, random_state = 42, shuffle = True)\n",
    "\n",
    "\n",
    "X_train, X_test = torch.tensor(df_train.drop('188', axis=1).values, dtype=torch.double), torch.tensor(df_test.drop('188', axis=1).values, dtype=torch.double)\n",
    "Y_train, Y_test = torch.tensor(df_train['188'].values, dtype=torch.double), torch.tensor(df_test['188'].values, dtype=torch.double) \n",
    "\n",
    "#display(X_train)\n",
    "display(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e9126e-1788-49d9-9957-3b45c924fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__() \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Model = nn.Sequential(\n",
    "            nn.Linear(1*187, 187, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(187, 187, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(187, 60, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 60, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 30, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 10, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1, bias=True),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "    \n",
    "    # To use the model, we pass it the input data. This executes the modelâ€™s forward, along with some background operations. \n",
    "    # DO NOT CALL model.forward() directly\n",
    "    def forward(self, input):\n",
    "        input = self.flatten(input)\n",
    "        logits = self.Model(input)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b27eab-d386-48f1-9d15-0b5db981ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, model, loss_eq, optimizer):   \n",
    "    size = features.size(dim=1) #187 (batch_size)\n",
    "    for X, Y in zip(features, labels):\n",
    "        X = X.view(1,187).float()\n",
    "        Y = Y.view(1,1).float()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #print(pred)\n",
    "        loss = loss_eq(pred, Y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if size % 100 == 0:\n",
    "            loss, current = loss.item(), size * len(X)\n",
    "\n",
    "\n",
    "def test(features, labels, model, loss_eq):\n",
    "\n",
    "    num_batches = features.size(dim=0) #Large Number (number of batches)\n",
    "    total_loss, correct_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in zip(features,labels):\n",
    "            X = X.view(1,187).float()\n",
    "            Y = Y.view(1,1).float()\n",
    "            pred = model(X)\n",
    "            # Perfomance Metric\n",
    "            \n",
    "            # Total Loss\n",
    "            total_loss += loss_eq(pred, Y).item()\n",
    "            \n",
    "            # Number of Times we had the correct prediction\n",
    "            correct_count += rounded_acc(pred, Y)\n",
    "\n",
    "    mean_loss = total_loss/num_batches\n",
    "    accuracy = correct_count/num_batches\n",
    "    print(f\"Accuracy: {(100*accuracy):>0.1f}%, Mean Loss: {mean_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "def rounded_acc(prediction, actual):\n",
    "    round_pred = torch.round(prediction).item()\n",
    "    actual = actual.item()\n",
    "    \n",
    "    if round_pred == actual:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1cf3f43-ed8e-4b50-acbb-0a456a01b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Accuracy: 76.3%, Mean Loss: 0.152314 \n",
      "\n",
      "Epoch: 2\n",
      "Accuracy: 83.4%, Mean Loss: 0.113034 \n",
      "\n",
      "Epoch: 3\n",
      "Accuracy: 85.2%, Mean Loss: 0.103992 \n",
      "\n",
      "Epoch: 4\n",
      "Accuracy: 88.3%, Mean Loss: 0.087232 \n",
      "\n",
      "Epoch: 5\n",
      "Accuracy: 88.3%, Mean Loss: 0.089488 \n",
      "\n",
      "Epoch: 6\n",
      "Accuracy: 89.5%, Mean Loss: 0.081636 \n",
      "\n",
      "Epoch: 7\n",
      "Accuracy: 91.3%, Mean Loss: 0.069791 \n",
      "\n",
      "Epoch: 8\n",
      "Accuracy: 91.4%, Mean Loss: 0.070190 \n",
      "\n",
      "Epoch: 9\n",
      "Accuracy: 91.7%, Mean Loss: 0.067175 \n",
      "\n",
      "Epoch: 10\n",
      "Accuracy: 91.8%, Mean Loss: 0.061400 \n",
      "\n",
      "Training Complete\n",
      "Training Error: \n",
      "Accuracy: 93.4%, Mean Loss: 0.053323 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "NNModel = NeuralNetwork().to()\n",
    "\n",
    "# Following Parameters are some of the involved in training of NN\n",
    "# Learning rate that NN updates model paramaters each epoch \n",
    "# Batch size is amount of input data to take before updating model parameters \n",
    "# Epochs is repititions of training cycle\n",
    "learning_rate = 1e-2\n",
    "epochs = 10\n",
    "num_layers = 1\n",
    "num_direction = 1\n",
    "batch = 187\n",
    "hidden_size = 1\n",
    "\n",
    "# Initialize Loss Function and Optimizer\n",
    "#loss_eq = nn.BCEWithLogitsLoss()\n",
    "loss_eq = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(NNModel.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch: {t+1}\")\n",
    "    train(X_train, Y_train, NNModel, loss_eq, optimizer)\n",
    "    test(X_test, Y_test, NNModel, loss_eq)\n",
    "print(\"Training Complete\")\n",
    "print(\"Training Error: \")\n",
    "test(X_train, Y_train, NNModel,loss_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e209bc-2301-4d59-87cd-f4ae4e491fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1109488-d61c-431e-ac09-bf47627e0f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
